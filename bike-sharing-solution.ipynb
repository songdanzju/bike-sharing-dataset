{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet\n",
    "\t- instant: record index\n",
    "\t- dteday : date\n",
    "\t- season : season (1:springer, 2:summer, 3:fall, 4:winter)\n",
    "\t- yr : year (0: 2011, 1:2012)\n",
    "\t- mnth : month ( 1 to 12)\n",
    "\t- hr : hour (0 to 23)\n",
    "\t- holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n",
    "\t- weekday : day of the week\n",
    "\t- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "\t+ weathersit : \n",
    "\t\t- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "\t\t- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "\t\t- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "\t\t- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "\t- temp : Normalized temperature in Celsius. The values are divided to 41 (max)\n",
    "\t- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)\n",
    "\t- hum: Normalized humidity. The values are divided to 100 (max)\n",
    "\t- windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "\t- casual: count of casual users\n",
    "\t- registered: count of registered users\n",
    "\t- cnt: count of total rental bikes including both casual and registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bike-sharing-solution\\nfunction: machine learning Bike Sharing dataset\\nML modules:\\n- DecisionTreeRegressor\\n- ExtraTreesRegressor\\n- RandomForestRegressor\\n- GradientBoostingRegressor\\n- SVR\\n- GridSearchCV\\n- metrics.mean_squared_error\\ndata sets:\\n- https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\\nusage: \\n\\nreferences:\\n- https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*- coding: utf8 -*-\n",
    "#@author: songdan_zju@126.com\n",
    "\"\"\"bike-sharing-solution\n",
    "function: machine learning Bike Sharing dataset\n",
    "ML modules:\n",
    "- DecisionTreeRegressor\n",
    "- ExtraTreesRegressor\n",
    "- RandomForestRegressor\n",
    "- GradientBoostingRegressor\n",
    "- SVR\n",
    "- GridSearchCV\n",
    "- metrics.mean_squared_error\n",
    "data sets:\n",
    "- https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
    "usage: \n",
    "\n",
    "references:\n",
    "- https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble.forest import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(fn):\n",
    "    \"\"\" simply read csv file with pandas read_csv\n",
    "        Parameters\n",
    "        ----------\n",
    "        fn: csv file name\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        df: pandas dataframe\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(fn)\n",
    "#     print('df.head(5)\\n', df.head(5))\n",
    "    return df\n",
    "\n",
    "# fn = 'Bike-Sharing-Dataset/hour.csv'\n",
    "# df = load_data(fn)\n",
    "# df['temp'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_df(df):\n",
    "    \"\"\" check if there are missing entries in df\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pandas dataframe\n",
    "    \"\"\"\n",
    "#    print(df[df.isnull().any(axis=1)])\n",
    "    print(df.isnull().any(axis=0))\n",
    "    assert df[df.isnull().any(axis=1)].index.tolist()==[]\n",
    "    \n",
    "# fn = 'Bike-Sharing-Dataset/hour.csv'\n",
    "# df = load_data(fn)\n",
    "# df.isnull().any(axis=0)\n",
    "# df.isnull().any(axis=1)\n",
    "# check_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\" generate new informative features for df\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pandas dataframe of features\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        df: feature engineered dataframe\n",
    "    \"\"\"\n",
    "    def cat_hr(x):\n",
    "        \"\"\"function for categorizing hr (by building decision tree)\"\"\"\n",
    "        if x['hr'] <= 6.5:\n",
    "            return 0\n",
    "        elif x['hr'] <= 8.5:\n",
    "            return 1\n",
    "        elif x['hr'] <= 15.5:\n",
    "            return 2\n",
    "        elif x['hr'] <= 19.5:\n",
    "            return 3\n",
    "        elif x['hr'] <= 21.5:\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "    \n",
    "    def cat_temp(x):\n",
    "        \"\"\"simple function for categrizing temp (by building decision tree)\"\"\"\n",
    "        if x['temp'] <= 0.27:\n",
    "            return 0\n",
    "        elif x['temp'] <= 0.35:\n",
    "            return 1\n",
    "        elif x['temp'] <= 0.69:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    \n",
    "    def cat_hum(x):\n",
    "        \"\"\"simple function for categrizing hum (by building decision tree)\"\"\"\n",
    "        if x['hum'] <= 0.435:\n",
    "            return 0\n",
    "        elif x['hum'] <= 0.625:\n",
    "            return 1\n",
    "        elif x['hum'] <= 0.855:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    \n",
    "    # parse dteday feature to generate new features\n",
    "    dt = pd.DatetimeIndex(df['dteday'])\n",
    "    df.set_index(dt, inplace=True)\n",
    "#    df['date'] = dt.date\n",
    "    df['day'] = dt.day\n",
    "    df['month'] = dt.month\n",
    "    df['year'] = dt.year\n",
    "#    df['hour'] = dt.hour\n",
    "    df['dow'] = dt.dayofweek\n",
    "    df['woy'] = dt.weekofyear\n",
    "    \n",
    "    # the peak_hr distribution ,\n",
    "    # on working day or not, the peak_hr is different\n",
    "    df['peak_hr'] = df[['hr', 'workingday']].apply(\n",
    "        lambda x: (0, 1)[(x['workingday']==1 and (7<=x['hr']<=9 or 17<=x['hr']<= 19)) or \n",
    "                         (x['workingday']==0 and 10<=x['hr']<=19)], axis = 1)\n",
    "    \n",
    "    # comfortable condition for riding, \n",
    "    # consider two conditions('atemp', 'windspeed') to categorize comfortable condition or not\n",
    "    df['atemp_windspeed'] = df[['atemp', 'windspeed']].apply(\n",
    "        lambda x: (0, 1)[(0.2537<=x['windspeed']<=0.2537 or 0.3284<=x['windspeed']<= 0.4925) and \n",
    "                         (0.6212<=x['atemp']<=0.6667)], axis = 1)\n",
    "    \n",
    "    # categorize 'hr', 'hum' and 'temp' with decision trees, done offline\n",
    "    # transform Numerical data to Categorical data\n",
    "    df['hr_cat'] = df[['hr']].apply(cat_hr, axis=1)\n",
    "    df['temp_cat'] = df[['temp']].apply(cat_temp, axis=1)\n",
    "    df['hum_cat'] = df[['hum']].apply(cat_hum, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(df, features):\n",
    "    \"\"\" split df[features] into train and test set with ShuffleSplit\n",
    "        it also generates a new feature 'cnt_season' by grouping counts of four seasons\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pandas dataframe\n",
    "        features: a list of columns of df, the set of features in train set\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        df: dataframe + 'cnt_season'\n",
    "        X_train, X_test, y_train, y_test: train set and test set for 'cnt' column\n",
    "        y_train_cas, y_test_cas, y_train_reg, y_test_reg: train and test sets for 'casual' and 'registered' columns (not used in this study)\n",
    "        time_test: datetime information of test set, for writing prediction results\n",
    "    \"\"\"\n",
    "    ss = cross_validation.ShuffleSplit(len(df), n_iter=1, test_size=0.1, random_state=1234)\n",
    "    for ind_train, ind_test in ss:\n",
    "        # add a cnt_season column using groupby and join\n",
    "        if 'cnt_season' not in df:\n",
    "            season_gb = df.ix[ind_train, :].groupby('season')[['cnt']].agg(sum)\n",
    "            season_gb.columns = ['cnt_season']\n",
    "            df = df.join(season_gb, on='season')\n",
    "        X_train = df.ix[ind_train, features].as_matrix()\n",
    "        X_test = df.ix[ind_test, features].as_matrix()\n",
    "        y_train = np.log1p(df.ix[ind_train, 'cnt'].as_matrix())\n",
    "        y_test = np.log1p(df.ix[ind_test, 'cnt'].as_matrix())\n",
    "        y_train_cas = np.log1p(df.ix[ind_train, 'casual'].as_matrix())\n",
    "        y_train_reg = np.log1p(df.ix[ind_train, 'registered'].as_matrix())\n",
    "        y_test_cas = np.log1p(df.ix[ind_test, 'casual'].as_matrix())\n",
    "        y_test_reg = np.log1p(df.ix[ind_test, 'registered'].as_matrix())\n",
    "        time_test = df.ix[ind_test, ['dteday', 'mnth', 'hr']].as_matrix()\n",
    "    return df, X_train, X_test, y_train, y_test, y_train_cas, y_test_cas, y_train_reg, y_test_reg, time_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_evaluate(est, X_train, y_train, X_test, y_test):\n",
    "    \"\"\" train/fit model on train set, predict test set, then calculate MSE\n",
    "        Parameters\n",
    "        ----------\n",
    "        est: sklearn estimator / regressor\n",
    "        X_train, y_train, X_test, y_test: train set and test set\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: prediction of test set\n",
    "        mse: MSE of y_pred vs. y_test\n",
    "    \"\"\"\n",
    "    est.fit(X_train, y_train)\n",
    "    y_pred = est.predict(X_test)\n",
    "    mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    return y_pred, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_run(fn, features, type):\n",
    "    \"\"\" load dataset, build feature set, and do learning\n",
    "        Parameters\n",
    "        ----------\n",
    "        fn: file name of dataset\n",
    "        features: a list of list, each of which is a feature list for different models\n",
    "        type: str for indicating feature set\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        predictions and feature-engineered dataset are saved to files\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "    print('test_run ' + type)\n",
    "    df = load_data(fn)\n",
    "    check_df(df)\n",
    "    df = feature_engineering(df)\n",
    "    \n",
    "    print(df.columns)\n",
    "#    print(df.head())\n",
    "#    print(df.groupby(['peak_hr'])['cnt'].agg(sum))\n",
    "    y_pred_list = []\n",
    "    for i, est in enumerate((\n",
    "        DecisionTreeRegressor(min_samples_split=20),\n",
    "        ExtraTreesRegressor(n_estimators=100, max_depth=None, min_samples_split=2, random_state=1234),\n",
    "        RandomForestRegressor(n_estimators=1000, max_depth=15, random_state=1234, min_samples_split=3, n_jobs=-1),\n",
    "        GradientBoostingRegressor(n_estimators=150, max_depth=10, random_state=0, min_samples_leaf=20, learning_rate=0.1, subsample=0.7, loss='ls'),\n",
    "        svm.SVR(C=30)\n",
    "        )):\n",
    "#        print(features[i])\n",
    "        df, X_train, X_test, y_train, y_test, y_train_cas, y_test_cas, y_train_reg, y_test_reg, time_test = split_data(df, features=features[i])\n",
    "        y_pred, mse = predict_evaluate(est, X_train, y_train, X_test, y_test)\n",
    "        est_name = str(est).split('(')[0]\n",
    "        print(type, est_name, np.round(mse, 4))\n",
    "        \"\"\" feature importance\n",
    "        if est_name != 'SVR':\n",
    "            # print out feature importance\n",
    "            sfi = sorted([(x[0], float('%.4f'%x[1])) for x in zip(features[i], est.feature_importances_)], key=lambda x: x[1], reverse=True)\n",
    "            print(sfi)\n",
    "            print([x[0] for x in sfi])\n",
    "        \"\"\"\n",
    "        y_pred_list.append([est_name, mse, y_pred])\n",
    "\n",
    "    # blending models\n",
    "    y_pred_blend = np.log1p(.2*(np.exp(y_pred_list[2][2])-1) + .8*(np.exp(y_pred_list[3][2])-1))\n",
    "    print(type+' blending: 0.2*'+y_pred_list[2][0]+' + 0.8*'+y_pred_list[3][0], metrics.mean_squared_error(y_test, y_pred_blend).round(4))\n",
    "    y_pred_blend = np.log1p(.3*(np.exp(y_pred_list[1][2])-1) + .7*(np.exp(y_pred_list[3][2])-1))\n",
    "    print(type+' blending: 0.3*'+y_pred_list[1][0]+' + 0.7*'+y_pred_list[3][0], metrics.mean_squared_error(y_test, y_pred_blend).round(4))\n",
    "    y_pred_blend = np.log1p(.3*(np.exp(y_pred_list[3][2])-1) + .7*(np.exp(y_pred_list[4][2])-1))\n",
    "    print(type+ ' blending: 0.2*'+y_pred_list[3][0]+' + 0.8*'+y_pred_list[4][0], metrics.mean_squared_error(y_test, y_pred_blend).round(4))\n",
    "    y_pred_blend = np.log1p(.6*(np.exp(y_pred_list[3][2])-1) + .4*(np.exp(y_pred_list[4][2])-1))\n",
    "    print(type+ ' blending: 0.6*'+y_pred_list[3][0]+' + 0.4*'+y_pred_list[4][0], metrics.mean_squared_error(y_test, y_pred_blend).round(4))\n",
    "    dff = pd.DataFrame({'datetime': time_test[:, 0], 'mnth': time_test[:, 1], 'hr': time_test[:, 2], 'cnt': np.expm1(y_test), 'prediction': y_pred_blend})\n",
    "    dff.to_csv('./out/prediction_blended.csv', index = False, columns=['datetime', 'mnth', 'hr', 'cnt', 'prediction'])\n",
    "    print('blended predictions saved in ./out/prediction_blended.csv')\n",
    "    df.to_csv('./Bike-Sharing-Dataset/hour_ext.csv')\n",
    "    print('extended dataset saved in ./Bike-Sharing-Dataset/hour_ext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_est(fn, features, est, param, outfn):\n",
    "    \"\"\" hyperparameter tuning for models using GridSearchCV\n",
    "        Parameters\n",
    "        ----------\n",
    "        fn: file name of dataset\n",
    "        features: feature set for different models\n",
    "        est: sklearn estimator / regressor\n",
    "        param: set of parameters to be tuned\n",
    "        outfn: file name for storing predication made by the model with best tuned params\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        best_params_: best tuned params\n",
    "    \"\"\"\n",
    "    est_name = str(est).split('(')[0]\n",
    "    print('grid_search_est --', est_name)\n",
    "    df = load_data(fn)\n",
    "    check_df(df)\n",
    "    df = feature_engineering(df)\n",
    "    df, X_train, X_test, y_train, y_test, y_train_cas, y_test_cas, y_train_reg, y_test_reg, time_test = split_data(df, features=features)\n",
    "\n",
    "    # this may take a while .....\n",
    "    gs_cv = GridSearchCV(est, param, n_jobs=4, verbose=2)\n",
    "    gs_cv.fit(X_train, y_train)\n",
    "     \n",
    "    result = gs_cv.predict(X_test)\n",
    "    mse_test = metrics.mean_squared_error(y_test, result)\n",
    "    print('grid_search_est --', est_name, '-- best params: ', gs_cv.best_params_)\n",
    "    print(\"Grid scores on development set:\")\n",
    "    for params, mean_score, scores in gs_cv.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std() * 2, params))\n",
    "    print('grid_search_est --', est_name, '-- mse on test set: ', mse_test)\n",
    "    result = np.expm1(result)\n",
    "    dff = pd.DataFrame({'datetime': time_test[:, 0], 'mnth': time_test[:, 1], 'hr': time_test[:, 2], 'cnt': np.expm1(y_test), 'prediction': result})\n",
    "    dff.to_csv(outfn, index = False, columns=['datetime', 'mnth', 'hr', 'cnt', 'prediction'])\n",
    "    print('grid_search_est --', est_name, '-- predictions saved in', outfn)\n",
    "    \n",
    "    return gs_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_search_est -- GradientBoostingRegressor\n",
      "instant       False\n",
      "dteday        False\n",
      "season        False\n",
      "yr            False\n",
      "mnth          False\n",
      "hr            False\n",
      "holiday       False\n",
      "weekday       False\n",
      "workingday    False\n",
      "weathersit    False\n",
      "temp          False\n",
      "atemp         False\n",
      "hum           False\n",
      "windspeed     False\n",
      "casual        False\n",
      "registered    False\n",
      "cnt           False\n",
      "dtype: bool\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/songdanzju/anaconda/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "/Users/songdanzju/anaconda/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=0.1, max_depth=5, min_samples_leaf=5 ..............\n",
      "[CV] learning_rate=0.1, max_depth=5, min_samples_leaf=5 ..............\n",
      "[CV] learning_rate=0.1, max_depth=5, min_samples_leaf=5 ..............\n",
      "[CV] learning_rate=0.1, max_depth=5, min_samples_leaf=10 .............\n",
      "[CV] ..... learning_rate=0.1, max_depth=5, min_samples_leaf=5 -   8.2s\n",
      "[CV] learning_rate=0.1, max_depth=5, min_samples_leaf=10 .............\n",
      "[CV] ..... learning_rate=0.1, max_depth=5, min_samples_leaf=5 -   8.2s\n",
      "[CV] learning_rate=0.1, max_depth=5, min_samples_leaf=10 .............\n",
      "[CV] ..... learning_rate=0.1, max_depth=5, min_samples_leaf=5 -   8.3s\n",
      "[CV] learning_rate=0.1, max_depth=5, min_samples_leaf=20 .............\n",
      "[CV] .... learning_rate=0.1, max_depth=5, min_samples_leaf=10 -   8.5s\n",
      "[CV] learning_rate=0.1, max_depth=5, min_samples_leaf=20 .............\n",
      "[CV] .... learning_rate=0.1, max_depth=5, min_samples_leaf=10 -   9.6s\n",
      "[CV] learning_rate=0.1, max_depth=5, min_samples_leaf=20 .............\n",
      "[CV] .... learning_rate=0.1, max_depth=5, min_samples_leaf=20 -   9.9s\n",
      "[CV] .... learning_rate=0.1, max_depth=5, min_samples_leaf=10 -  10.0s\n",
      "[CV] learning_rate=0.1, max_depth=10, min_samples_leaf=5 .............\n",
      "[CV] learning_rate=0.1, max_depth=10, min_samples_leaf=5 .............\n",
      "[CV] .... learning_rate=0.1, max_depth=5, min_samples_leaf=20 -  10.0s\n",
      "[CV] learning_rate=0.1, max_depth=10, min_samples_leaf=5 .............\n",
      "[CV] .... learning_rate=0.1, max_depth=5, min_samples_leaf=20 -   6.6s\n",
      "[CV] learning_rate=0.1, max_depth=10, min_samples_leaf=10 ............\n",
      "[CV] .... learning_rate=0.1, max_depth=10, min_samples_leaf=5 -  23.5s\n",
      "[CV] learning_rate=0.1, max_depth=10, min_samples_leaf=10 ............\n",
      "[CV] .... learning_rate=0.1, max_depth=10, min_samples_leaf=5 -  24.1s\n",
      "[CV] learning_rate=0.1, max_depth=10, min_samples_leaf=10 ............\n",
      "[CV] .... learning_rate=0.1, max_depth=10, min_samples_leaf=5 -  24.5s\n",
      "[CV] learning_rate=0.1, max_depth=10, min_samples_leaf=20 ............\n",
      "[CV] ... learning_rate=0.1, max_depth=10, min_samples_leaf=10 -  19.3s\n",
      "[CV] learning_rate=0.1, max_depth=10, min_samples_leaf=20 ............\n",
      "[CV] ... learning_rate=0.1, max_depth=10, min_samples_leaf=20 -  16.7s\n",
      "[CV] learning_rate=0.1, max_depth=10, min_samples_leaf=20 ............\n",
      "[CV] ... learning_rate=0.1, max_depth=10, min_samples_leaf=20 -  16.8s\n",
      "[CV] learning_rate=0.1, max_depth=15, min_samples_leaf=5 .............\n",
      "[CV] ... learning_rate=0.1, max_depth=10, min_samples_leaf=10 -  20.2s\n",
      "[CV] learning_rate=0.1, max_depth=15, min_samples_leaf=5 .............\n",
      "[CV] ... learning_rate=0.1, max_depth=10, min_samples_leaf=10 -  20.5s\n",
      "[CV] learning_rate=0.1, max_depth=15, min_samples_leaf=5 .............\n",
      "[CV] ... learning_rate=0.1, max_depth=10, min_samples_leaf=20 -  14.7s\n",
      "[CV] learning_rate=0.1, max_depth=15, min_samples_leaf=10 ............\n",
      "[CV] .... learning_rate=0.1, max_depth=15, min_samples_leaf=5 -  43.1s\n",
      "[CV] learning_rate=0.1, max_depth=15, min_samples_leaf=10 ............\n",
      "[CV] ... learning_rate=0.1, max_depth=15, min_samples_leaf=10 -  30.6s\n",
      "[CV] learning_rate=0.1, max_depth=15, min_samples_leaf=10 ............\n",
      "[CV] .... learning_rate=0.1, max_depth=15, min_samples_leaf=5 -  45.2s\n",
      "[CV] learning_rate=0.1, max_depth=15, min_samples_leaf=20 ............\n",
      "[CV] .... learning_rate=0.1, max_depth=15, min_samples_leaf=5 -  45.5s\n",
      "[CV] learning_rate=0.1, max_depth=15, min_samples_leaf=20 ............\n",
      "[CV] ... learning_rate=0.1, max_depth=15, min_samples_leaf=20 -  24.8s\n",
      "[CV] learning_rate=0.1, max_depth=15, min_samples_leaf=20 ............\n",
      "[CV] ... learning_rate=0.1, max_depth=15, min_samples_leaf=20 -  23.8s\n",
      "[CV] learning_rate=0.05, max_depth=5, min_samples_leaf=5 .............\n",
      "[CV] ... learning_rate=0.1, max_depth=15, min_samples_leaf=10 -  33.7s\n",
      "[CV] learning_rate=0.05, max_depth=5, min_samples_leaf=5 .............\n",
      "[CV] ... learning_rate=0.1, max_depth=15, min_samples_leaf=10 -  32.7s\n",
      "[CV] learning_rate=0.05, max_depth=5, min_samples_leaf=5 .............\n",
      "[CV] .... learning_rate=0.05, max_depth=5, min_samples_leaf=5 -   6.7s\n",
      "[CV] learning_rate=0.05, max_depth=5, min_samples_leaf=10 ............\n",
      "[CV] .... learning_rate=0.05, max_depth=5, min_samples_leaf=5 -   7.7s\n",
      "[CV] learning_rate=0.05, max_depth=5, min_samples_leaf=10 ............\n",
      "[CV] .... learning_rate=0.05, max_depth=5, min_samples_leaf=5 -   7.7s\n",
      "[CV] learning_rate=0.05, max_depth=5, min_samples_leaf=10 ............\n",
      "[CV] ... learning_rate=0.05, max_depth=5, min_samples_leaf=10 -   8.0s\n",
      "[CV] learning_rate=0.05, max_depth=5, min_samples_leaf=20 ............\n",
      "[CV] ... learning_rate=0.05, max_depth=5, min_samples_leaf=10 -   7.2s\n",
      "[CV] learning_rate=0.05, max_depth=5, min_samples_leaf=20 ............\n",
      "[CV] ... learning_rate=0.05, max_depth=5, min_samples_leaf=10 -   7.2s\n",
      "[CV] learning_rate=0.05, max_depth=5, min_samples_leaf=20 ............\n",
      "[CV] ... learning_rate=0.05, max_depth=5, min_samples_leaf=20 -   6.6s\n",
      "[CV] learning_rate=0.05, max_depth=10, min_samples_leaf=5 ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  2.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... learning_rate=0.05, max_depth=5, min_samples_leaf=20 -   7.2s\n",
      "[CV] learning_rate=0.05, max_depth=10, min_samples_leaf=5 ............\n",
      "[CV] ... learning_rate=0.05, max_depth=5, min_samples_leaf=20 -   7.2s\n",
      "[CV] learning_rate=0.05, max_depth=10, min_samples_leaf=5 ............\n",
      "[CV] ... learning_rate=0.1, max_depth=15, min_samples_leaf=20 -  28.0s\n",
      "[CV] learning_rate=0.05, max_depth=10, min_samples_leaf=10 ...........\n",
      "[CV] .. learning_rate=0.05, max_depth=10, min_samples_leaf=10 -  23.3s\n",
      "[CV] learning_rate=0.05, max_depth=10, min_samples_leaf=10 ...........\n",
      "[CV] ... learning_rate=0.05, max_depth=10, min_samples_leaf=5 -  30.1s\n",
      "[CV] learning_rate=0.05, max_depth=10, min_samples_leaf=10 ...........\n",
      "[CV] ... learning_rate=0.05, max_depth=10, min_samples_leaf=5 -  29.4s\n",
      "[CV] learning_rate=0.05, max_depth=10, min_samples_leaf=20 ...........\n",
      "[CV] ... learning_rate=0.05, max_depth=10, min_samples_leaf=5 -  30.1s\n",
      "[CV] learning_rate=0.05, max_depth=10, min_samples_leaf=20 ...........\n",
      "[CV] .. learning_rate=0.05, max_depth=10, min_samples_leaf=10 -  23.3s\n",
      "[CV] learning_rate=0.05, max_depth=10, min_samples_leaf=20 ...........\n",
      "[CV] .. learning_rate=0.05, max_depth=10, min_samples_leaf=10 -  23.4s\n",
      "[CV] learning_rate=0.05, max_depth=15, min_samples_leaf=5 ............\n",
      "[CV] .. learning_rate=0.05, max_depth=10, min_samples_leaf=20 -  18.3s\n",
      "[CV] learning_rate=0.05, max_depth=15, min_samples_leaf=5 ............\n",
      "[CV] .. learning_rate=0.05, max_depth=10, min_samples_leaf=20 -  18.3s\n",
      "[CV] learning_rate=0.05, max_depth=15, min_samples_leaf=5 ............\n",
      "[CV] .. learning_rate=0.05, max_depth=10, min_samples_leaf=20 -  16.7s\n",
      "[CV] learning_rate=0.05, max_depth=15, min_samples_leaf=10 ...........\n",
      "[CV] .. learning_rate=0.05, max_depth=15, min_samples_leaf=10 -  38.8s\n",
      "[CV] learning_rate=0.05, max_depth=15, min_samples_leaf=10 ...........\n",
      "[CV] ... learning_rate=0.05, max_depth=15, min_samples_leaf=5 -  57.9s\n",
      "[CV] learning_rate=0.05, max_depth=15, min_samples_leaf=10 ...........\n",
      "[CV] ... learning_rate=0.05, max_depth=15, min_samples_leaf=5 -  59.0s\n",
      "[CV] learning_rate=0.05, max_depth=15, min_samples_leaf=20 ...........\n",
      "[CV] ... learning_rate=0.05, max_depth=15, min_samples_leaf=5 -  58.7s\n",
      "[CV] learning_rate=0.05, max_depth=15, min_samples_leaf=20 ...........\n",
      "[CV] .. learning_rate=0.05, max_depth=15, min_samples_leaf=20 -  25.5s\n",
      "[CV] learning_rate=0.05, max_depth=15, min_samples_leaf=20 ...........\n",
      "[CV] .. learning_rate=0.05, max_depth=15, min_samples_leaf=20 -  25.8s\n",
      "[CV] .. learning_rate=0.05, max_depth=15, min_samples_leaf=10 -  34.8s\n",
      "[CV] .. learning_rate=0.05, max_depth=15, min_samples_leaf=10 -  33.2s\n",
      "[CV] .. learning_rate=0.05, max_depth=15, min_samples_leaf=20 -  14.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  54 out of  54 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_search_est -- GradientBoostingRegressor -- best params:  {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_leaf': 20}\n",
      "Grid scores on development set:\n",
      "0.958 (+/-0.003) for {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 5}\n",
      "0.958 (+/-0.003) for {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 10}\n",
      "0.958 (+/-0.002) for {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 20}\n",
      "0.958 (+/-0.003) for {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_leaf': 5}\n",
      "0.960 (+/-0.003) for {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_leaf': 10}\n",
      "0.960 (+/-0.003) for {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_leaf': 20}\n",
      "0.953 (+/-0.002) for {'learning_rate': 0.1, 'max_depth': 15, 'min_samples_leaf': 5}\n",
      "0.957 (+/-0.003) for {'learning_rate': 0.1, 'max_depth': 15, 'min_samples_leaf': 10}\n",
      "0.960 (+/-0.003) for {'learning_rate': 0.1, 'max_depth': 15, 'min_samples_leaf': 20}\n",
      "0.952 (+/-0.003) for {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_leaf': 5}\n",
      "0.952 (+/-0.003) for {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_leaf': 10}\n",
      "0.952 (+/-0.004) for {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_leaf': 20}\n",
      "0.958 (+/-0.003) for {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_leaf': 5}\n",
      "0.959 (+/-0.004) for {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_leaf': 10}\n",
      "0.960 (+/-0.003) for {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_leaf': 20}\n",
      "0.953 (+/-0.002) for {'learning_rate': 0.05, 'max_depth': 15, 'min_samples_leaf': 5}\n",
      "0.958 (+/-0.004) for {'learning_rate': 0.05, 'max_depth': 15, 'min_samples_leaf': 10}\n",
      "0.960 (+/-0.003) for {'learning_rate': 0.05, 'max_depth': 15, 'min_samples_leaf': 20}\n",
      "grid_search_est -- GradientBoostingRegressor -- mse on test set:  0.0763896263245\n",
      "grid_search_est -- GradientBoostingRegressor -- predictions saved in ./out/prediction_gs_gbm.csv\n",
      "grid_search_est -- SVR\n",
      "instant       False\n",
      "dteday        False\n",
      "season        False\n",
      "yr            False\n",
      "mnth          False\n",
      "hr            False\n",
      "holiday       False\n",
      "weekday       False\n",
      "workingday    False\n",
      "weathersit    False\n",
      "temp          False\n",
      "atemp         False\n",
      "hum           False\n",
      "windspeed     False\n",
      "casual        False\n",
      "registered    False\n",
      "cnt           False\n",
      "dtype: bool\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/songdanzju/anaconda/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "/Users/songdanzju/anaconda/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] C=1 .............................................................\n",
      "[CV] C=1 .............................................................\n",
      "[CV] C=1 .............................................................\n",
      "[CV] C=10 ............................................................\n",
      "[CV] .................................................... C=1 -  11.1s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] .................................................... C=1 -  11.1s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] .................................................... C=1 -  11.3s\n",
      "[CV] C=20 ............................................................\n",
      "[CV] ................................................... C=10 -  33.3s\n",
      "[CV] C=20 ............................................................\n",
      "[CV] ................................................... C=10 -  32.1s\n",
      "[CV] C=20 ............................................................\n",
      "[CV] ................................................... C=10 -  33.6s\n",
      "[CV] C=30 ............................................................\n",
      "[CV] ................................................... C=20 -  57.5s\n",
      "[CV] C=30 ............................................................\n",
      "[CV] ................................................... C=20 -  57.5s\n",
      "[CV] C=30 ............................................................\n",
      "[CV] ................................................... C=20 -  56.9s\n",
      "[CV] C=40 ............................................................\n",
      "[CV] ................................................... C=30 - 1.3min\n",
      "[CV] C=40 ............................................................\n",
      "[CV] ................................................... C=30 - 1.3min\n",
      "[CV] C=40 ............................................................\n",
      "[CV] ................................................... C=30 - 1.2min\n",
      "[CV] ................................................... C=40 - 1.5min\n",
      "[CV] ................................................... C=40 - 1.4min\n",
      "[CV] ................................................... C=40 - 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  15 out of  15 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_search_est -- SVR -- best params:  {'C': 30}\n",
      "Grid scores on development set:\n",
      "0.946 (+/-0.005) for {'C': 1}\n",
      "0.954 (+/-0.004) for {'C': 10}\n",
      "0.954 (+/-0.004) for {'C': 20}\n",
      "0.954 (+/-0.004) for {'C': 30}\n",
      "0.954 (+/-0.004) for {'C': 40}\n",
      "grid_search_est -- SVR -- mse on test set:  0.0809414550243\n",
      "grid_search_est -- SVR -- predictions saved in ./out/prediction_gs_svr.csv\n",
      "grid_search_est -- RandomForestRegressor\n",
      "instant       False\n",
      "dteday        False\n",
      "season        False\n",
      "yr            False\n",
      "mnth          False\n",
      "hr            False\n",
      "holiday       False\n",
      "weekday       False\n",
      "workingday    False\n",
      "weathersit    False\n",
      "temp          False\n",
      "atemp         False\n",
      "hum           False\n",
      "windspeed     False\n",
      "casual        False\n",
      "registered    False\n",
      "cnt           False\n",
      "dtype: bool\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/songdanzju/anaconda/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "/Users/songdanzju/anaconda/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] max_depth=5, min_samples_split=3, n_estimators=500 ..............\n",
      "[CV] max_depth=5, min_samples_split=3, n_estimators=500 ..............\n",
      "[CV] max_depth=5, min_samples_split=3, n_estimators=500 ..............\n",
      "[CV] max_depth=5, min_samples_split=3, n_estimators=1000 .............\n",
      "[CV] ..... max_depth=5, min_samples_split=3, n_estimators=500 -  10.0s\n",
      "[CV] max_depth=5, min_samples_split=3, n_estimators=1000 .............\n",
      "[CV] ..... max_depth=5, min_samples_split=3, n_estimators=500 -  10.5s\n",
      "[CV] max_depth=5, min_samples_split=3, n_estimators=1000 .............\n",
      "[CV] ..... max_depth=5, min_samples_split=3, n_estimators=500 -  10.5s\n",
      "[CV] max_depth=5, min_samples_split=5, n_estimators=500 ..............\n",
      "[CV] .... max_depth=5, min_samples_split=3, n_estimators=1000 -  23.3s\n",
      "[CV] max_depth=5, min_samples_split=5, n_estimators=500 ..............\n",
      "[CV] ..... max_depth=5, min_samples_split=5, n_estimators=500 -  12.9s\n",
      "[CV] max_depth=5, min_samples_split=5, n_estimators=500 ..............\n",
      "[CV] .... max_depth=5, min_samples_split=3, n_estimators=1000 -  27.8s\n",
      "[CV] max_depth=5, min_samples_split=5, n_estimators=1000 .............\n",
      "[CV] ..... max_depth=5, min_samples_split=5, n_estimators=500 -  15.0s\n",
      "[CV] max_depth=5, min_samples_split=5, n_estimators=1000 .............\n",
      "[CV] ..... max_depth=5, min_samples_split=5, n_estimators=500 -  15.3s\n",
      "[CV] max_depth=5, min_samples_split=5, n_estimators=1000 .............\n",
      "[CV] .... max_depth=5, min_samples_split=3, n_estimators=1000 -  28.3s\n",
      "[CV] max_depth=5, min_samples_split=10, n_estimators=500 .............\n",
      "[CV] .... max_depth=5, min_samples_split=10, n_estimators=500 -  11.1s\n",
      "[CV] max_depth=5, min_samples_split=10, n_estimators=500 .............\n",
      "[CV] .... max_depth=5, min_samples_split=5, n_estimators=1000 -  25.9s\n",
      "[CV] max_depth=5, min_samples_split=10, n_estimators=500 .............\n",
      "[CV] .... max_depth=5, min_samples_split=5, n_estimators=1000 -  26.2s\n",
      "[CV] max_depth=5, min_samples_split=10, n_estimators=1000 ............\n",
      "[CV] .... max_depth=5, min_samples_split=5, n_estimators=1000 -  26.2s\n",
      "[CV] max_depth=5, min_samples_split=10, n_estimators=1000 ............\n",
      "[CV] .... max_depth=5, min_samples_split=10, n_estimators=500 -  15.0s\n",
      "[CV] max_depth=5, min_samples_split=10, n_estimators=1000 ............\n",
      "[CV] .... max_depth=5, min_samples_split=10, n_estimators=500 -  11.2s\n",
      "[CV] max_depth=10, min_samples_split=3, n_estimators=500 .............\n",
      "[CV] ... max_depth=5, min_samples_split=10, n_estimators=1000 -  26.4s\n",
      "[CV] max_depth=10, min_samples_split=3, n_estimators=500 .............\n",
      "[CV] ... max_depth=5, min_samples_split=10, n_estimators=1000 -  26.9s\n",
      "[CV] max_depth=10, min_samples_split=3, n_estimators=500 .............\n",
      "[CV] ... max_depth=5, min_samples_split=10, n_estimators=1000 -  26.8s\n",
      "[CV] max_depth=10, min_samples_split=3, n_estimators=1000 ............\n",
      "[CV] .... max_depth=10, min_samples_split=3, n_estimators=500 -  24.0s\n",
      "[CV] max_depth=10, min_samples_split=3, n_estimators=1000 ............\n",
      "[CV] .... max_depth=10, min_samples_split=3, n_estimators=500 -  24.4s\n",
      "[CV] max_depth=10, min_samples_split=3, n_estimators=1000 ............\n",
      "[CV] .... max_depth=10, min_samples_split=3, n_estimators=500 -  24.4s\n",
      "[CV] max_depth=10, min_samples_split=5, n_estimators=500 .............\n",
      "[CV] .... max_depth=10, min_samples_split=5, n_estimators=500 -  29.8s\n",
      "[CV] max_depth=10, min_samples_split=5, n_estimators=500 .............\n",
      "[CV] ... max_depth=10, min_samples_split=3, n_estimators=1000 -  54.5s\n",
      "[CV] max_depth=10, min_samples_split=5, n_estimators=500 .............\n",
      "[CV] ... max_depth=10, min_samples_split=3, n_estimators=1000 -  60.0s\n",
      "[CV] max_depth=10, min_samples_split=5, n_estimators=1000 ............\n",
      "[CV] ... max_depth=10, min_samples_split=3, n_estimators=1000 -  57.3s\n",
      "[CV] max_depth=10, min_samples_split=5, n_estimators=1000 ............\n",
      "[CV] .... max_depth=10, min_samples_split=5, n_estimators=500 -  27.2s\n",
      "[CV] max_depth=10, min_samples_split=5, n_estimators=1000 ............\n",
      "[CV] .... max_depth=10, min_samples_split=5, n_estimators=500 -  27.3s\n",
      "[CV] max_depth=10, min_samples_split=10, n_estimators=500 ............\n",
      "[CV] ... max_depth=10, min_samples_split=10, n_estimators=500 -  22.2s\n",
      "[CV] max_depth=10, min_samples_split=10, n_estimators=500 ............\n",
      "[CV] ... max_depth=10, min_samples_split=5, n_estimators=1000 -  46.0s\n",
      "[CV] max_depth=10, min_samples_split=10, n_estimators=500 ............\n",
      "[CV] ... max_depth=10, min_samples_split=5, n_estimators=1000 -  47.1s\n",
      "[CV] max_depth=10, min_samples_split=10, n_estimators=1000 ...........\n",
      "[CV] ... max_depth=10, min_samples_split=10, n_estimators=500 -  24.3s\n",
      "[CV] max_depth=10, min_samples_split=10, n_estimators=1000 ...........\n",
      "[CV] ... max_depth=10, min_samples_split=5, n_estimators=1000 -  47.1s\n",
      "[CV] max_depth=10, min_samples_split=10, n_estimators=1000 ...........\n",
      "[CV] ... max_depth=10, min_samples_split=10, n_estimators=500 -  21.1s\n",
      "[CV] max_depth=15, min_samples_split=3, n_estimators=500 .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  3.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... max_depth=15, min_samples_split=3, n_estimators=500 -  36.4s\n",
      "[CV] max_depth=15, min_samples_split=3, n_estimators=500 .............\n",
      "[CV] .. max_depth=10, min_samples_split=10, n_estimators=1000 -  45.4s\n",
      "[CV] max_depth=15, min_samples_split=3, n_estimators=500 .............\n",
      "[CV] .. max_depth=10, min_samples_split=10, n_estimators=1000 -  45.6s\n",
      "[CV] max_depth=15, min_samples_split=3, n_estimators=1000 ............\n",
      "[CV] .. max_depth=10, min_samples_split=10, n_estimators=1000 -  45.6s\n",
      "[CV] max_depth=15, min_samples_split=3, n_estimators=1000 ............\n",
      "[CV] .... max_depth=15, min_samples_split=3, n_estimators=500 -  32.9s\n",
      "[CV] max_depth=15, min_samples_split=3, n_estimators=1000 ............\n",
      "[CV] .... max_depth=15, min_samples_split=3, n_estimators=500 -  32.6s\n",
      "[CV] max_depth=15, min_samples_split=5, n_estimators=500 .............\n",
      "[CV] .... max_depth=15, min_samples_split=5, n_estimators=500 -  30.8s\n",
      "[CV] max_depth=15, min_samples_split=5, n_estimators=500 .............\n",
      "[CV] ... max_depth=15, min_samples_split=3, n_estimators=1000 - 1.1min\n",
      "[CV] max_depth=15, min_samples_split=5, n_estimators=500 .............\n",
      "[CV] ... max_depth=15, min_samples_split=3, n_estimators=1000 - 1.1min\n",
      "[CV] max_depth=15, min_samples_split=5, n_estimators=1000 ............\n",
      "[CV] .... max_depth=15, min_samples_split=5, n_estimators=500 -  32.2s\n",
      "[CV] max_depth=15, min_samples_split=5, n_estimators=1000 ............\n",
      "[CV] ... max_depth=15, min_samples_split=3, n_estimators=1000 - 1.1min\n",
      "[CV] max_depth=15, min_samples_split=5, n_estimators=1000 ............\n",
      "[CV] .... max_depth=15, min_samples_split=5, n_estimators=500 -  32.8s\n",
      "[CV] max_depth=15, min_samples_split=10, n_estimators=500 ............\n",
      "[CV] ... max_depth=15, min_samples_split=10, n_estimators=500 -  29.0s\n",
      "[CV] max_depth=15, min_samples_split=10, n_estimators=500 ............\n",
      "[CV] ... max_depth=15, min_samples_split=5, n_estimators=1000 - 1.1min\n",
      "[CV] max_depth=15, min_samples_split=10, n_estimators=500 ............\n",
      "[CV] ... max_depth=15, min_samples_split=10, n_estimators=500 -  31.0s\n",
      "[CV] max_depth=15, min_samples_split=10, n_estimators=1000 ...........\n",
      "[CV] ... max_depth=15, min_samples_split=5, n_estimators=1000 - 1.1min\n",
      "[CV] max_depth=15, min_samples_split=10, n_estimators=1000 ...........\n",
      "[CV] ... max_depth=15, min_samples_split=5, n_estimators=1000 - 1.1min\n",
      "[CV] max_depth=15, min_samples_split=10, n_estimators=1000 ...........\n",
      "[CV] ... max_depth=15, min_samples_split=10, n_estimators=500 -  31.5s\n",
      "[CV] .. max_depth=15, min_samples_split=10, n_estimators=1000 -  48.4s\n",
      "[CV] .. max_depth=15, min_samples_split=10, n_estimators=1000 -  47.5s\n",
      "[CV] .. max_depth=15, min_samples_split=10, n_estimators=1000 -  47.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  54 out of  54 | elapsed:  7.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_search_est -- RandomForestRegressor -- best params:  {'max_depth': 15, 'min_samples_split': 3, 'n_estimators': 1000}\n",
      "Grid scores on development set:\n",
      "0.871 (+/-0.010) for {'max_depth': 5, 'min_samples_split': 3, 'n_estimators': 500}\n",
      "0.871 (+/-0.009) for {'max_depth': 5, 'min_samples_split': 3, 'n_estimators': 1000}\n",
      "0.871 (+/-0.009) for {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 500}\n",
      "0.871 (+/-0.009) for {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 1000}\n",
      "0.871 (+/-0.009) for {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 500}\n",
      "0.871 (+/-0.009) for {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 1000}\n",
      "0.946 (+/-0.003) for {'max_depth': 10, 'min_samples_split': 3, 'n_estimators': 500}\n",
      "0.946 (+/-0.003) for {'max_depth': 10, 'min_samples_split': 3, 'n_estimators': 1000}\n",
      "0.946 (+/-0.003) for {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 500}\n",
      "0.946 (+/-0.003) for {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 1000}\n",
      "0.945 (+/-0.003) for {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 500}\n",
      "0.945 (+/-0.003) for {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 1000}\n",
      "0.953 (+/-0.003) for {'max_depth': 15, 'min_samples_split': 3, 'n_estimators': 500}\n",
      "0.953 (+/-0.003) for {'max_depth': 15, 'min_samples_split': 3, 'n_estimators': 1000}\n",
      "0.952 (+/-0.003) for {'max_depth': 15, 'min_samples_split': 5, 'n_estimators': 500}\n",
      "0.953 (+/-0.003) for {'max_depth': 15, 'min_samples_split': 5, 'n_estimators': 1000}\n",
      "0.951 (+/-0.003) for {'max_depth': 15, 'min_samples_split': 10, 'n_estimators': 500}\n",
      "0.952 (+/-0.003) for {'max_depth': 15, 'min_samples_split': 10, 'n_estimators': 1000}\n",
      "grid_search_est -- RandomForestRegressor -- mse on test set:  0.0952282475586\n",
      "grid_search_est -- RandomForestRegressor -- predictions saved in ./out/prediction_gs_rfr.csv\n",
      "test_run featureset2\n",
      "instant       False\n",
      "dteday        False\n",
      "season        False\n",
      "yr            False\n",
      "mnth          False\n",
      "hr            False\n",
      "holiday       False\n",
      "weekday       False\n",
      "workingday    False\n",
      "weathersit    False\n",
      "temp          False\n",
      "atemp         False\n",
      "hum           False\n",
      "windspeed     False\n",
      "casual        False\n",
      "registered    False\n",
      "cnt           False\n",
      "dtype: bool\n",
      "Index(['instant', 'dteday', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\n",
      "       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\n",
      "       'casual', 'registered', 'cnt', 'day', 'month', 'year', 'dow', 'woy',\n",
      "       'peak_hr', 'atemp_windspeed', 'hr_cat', 'temp_cat', 'hum_cat'],\n",
      "      dtype='object')\n",
      "featureset2 DecisionTreeRegressor 0.1448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/songdanzju/anaconda/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "/Users/songdanzju/anaconda/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureset2 ExtraTreesRegressor 0.0969\n",
      "featureset2 RandomForestRegressor 0.0953\n",
      "featureset2 GradientBoostingRegressor 0.0759\n",
      "featureset2 SVR 0.0809\n",
      "featureset2 blending: 0.2*RandomForestRegressor + 0.8*GradientBoostingRegressor 0.0768\n",
      "featureset2 blending: 0.3*ExtraTreesRegressor + 0.7*GradientBoostingRegressor 0.0762\n",
      "featureset2 blending: 0.2*GradientBoostingRegressor + 0.8*SVR 0.0735\n",
      "featureset2 blending: 0.6*GradientBoostingRegressor + 0.4*SVR 0.0709\n",
      "blended predictions saved in ./out/prediction_blended.csv\n",
      "extended dataset saved in ./Bike-Sharing-Dataset/hour_ext.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" current running result:\\ntest_run featureset2\\nIndex(['instant', 'dteday', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\\n       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\\n       'casual', 'registered', 'cnt', 'day', 'month', 'year', 'dow', 'woy',\\n       'peak_hr', 'atemp_windspeed', 'hr_cat', 'temp_cat', 'hum_cat'],\\n      dtype='object')\\nfeatureset2 DecisionTreeRegressor 0.1453\\nfeatureset2 ExtraTreesRegressor 0.0969\\nfeatureset2 RandomForestRegressor 0.0953\\nfeatureset2 GradientBoostingRegressor 0.0759\\nfeatureset2 SVR 0.0809\\nfeatureset2 blending: 0.2*RandomForestRegressor + 0.8*GradientBoostingRegressor 0.0768\\nfeatureset2 blending: 0.3*ExtraTreesRegressor + 0.7*GradientBoostingRegressor 0.0762\\nfeatureset2 blending: 0.2*GradientBoostingRegressor + 0.8*SVR 0.0735\\nfeatureset2 blending: 0.6*GradientBoostingRegressor + 0.4*SVR 0.0709\\nblended predictions saved in ./out/prediction_blended.csv\\nextended dataset saved in ./Bike-Sharing-Dataset/hour_ext.csv\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#    features = [['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'day', 'month', 'year', 'dow', 'woy', 'peak_hr', 'atemp_windspeed', 'hr_cat', 'temp_cat', 'hum_cat', 'cnt_season'], ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'day', 'month', 'year', 'dow', 'woy', 'peak_hr', 'atemp_windspeed', 'hr_cat', 'temp_cat', 'hum_cat', 'cnt_season'], ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'day', 'month', 'year', 'dow', 'woy', 'peak_hr', 'atemp_windspeed', 'hr_cat', 'temp_cat', 'hum_cat', 'cnt_season'], ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'day', 'month', 'year', 'dow', 'woy', 'peak_hr', 'atemp_windspeed', 'hr_cat', 'temp_cat', 'hum_cat', 'cnt_season'], ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']] #featureset1\n",
    "    features = [['hr', 'temp', 'peak_hr', 'dow', 'workingday', 'year', 'hum', 'woy', 'weathersit', 'season', 'atemp', 'temp_cat', 'yr'],\n",
    "                ['hr', 'hr_cat', 'peak_hr', 'workingday', 'temp_cat', 'cnt_season', 'temp', 'atemp', 'yr', 'weathersit', 'year', 'season', 'hum_cat', 'hum', 'dow', 'woy', 'weekday', 'mnth', 'windspeed'],\n",
    "                ['hr', 'hr_cat', 'temp', 'peak_hr', 'workingday', 'dow', 'atemp', 'woy', 'hum', 'year', 'yr', 'weathersit', 'season', 'day', 'windspeed', 'cnt_season', 'weekday', 'temp_cat'],\n",
    "                ['hr', 'woy', 'day', 'hum', 'dow', 'hr_cat', 'atemp', 'temp', 'workingday', 'windspeed', 'weekday', 'weathersit', 'peak_hr', 'holiday', 'year', 'month', 'yr', 'season', 'mnth', 'cnt_season'],\n",
    "                ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']]#featureset2\n",
    "    # GridSearchCV\n",
    "    param = {'learning_rate': [0.1, 0.05],\n",
    "             'max_depth': [5, 10, 15],\n",
    "             'min_samples_leaf': [5, 10, 20],\n",
    "             }\n",
    "    est = GradientBoostingRegressor(n_estimators=150)\n",
    "    grid_search_est(fn='./Bike-Sharing-Dataset/hour.csv', features=features[3], est=est, param=param, outfn='./out/prediction_gs_gbm.csv')\n",
    "    param = {'C': [1, 10, 20, 30, 40]\n",
    "             }\n",
    "    est = svm.SVR()\n",
    "    grid_search_est(fn='./Bike-Sharing-Dataset/hour.csv', features=features[4], est=est, param=param, outfn='./out/prediction_gs_svr.csv')\n",
    "    param = {'n_estimators': [500, 1000],\n",
    "             'max_depth': [5, 10, 15],\n",
    "             'min_samples_split': [3, 5, 10],\n",
    "             }\n",
    "    est = RandomForestRegressor(n_jobs=-1)\n",
    "    grid_search_est(fn='./Bike-Sharing-Dataset/hour.csv', features=features[2], est=est, param=param, outfn='./out/prediction_gs_rfr.csv')\n",
    "    \n",
    "    test_run(fn='./Bike-Sharing-Dataset/hour.csv', features=features, type='featureset2')\n",
    "    \n",
    "\"\"\" current running result:\n",
    "test_run featureset2\n",
    "Index(['instant', 'dteday', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\n",
    "       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\n",
    "       'casual', 'registered', 'cnt', 'day', 'month', 'year', 'dow', 'woy',\n",
    "       'peak_hr', 'atemp_windspeed', 'hr_cat', 'temp_cat', 'hum_cat'],\n",
    "      dtype='object')\n",
    "featureset2 DecisionTreeRegressor 0.1453\n",
    "featureset2 ExtraTreesRegressor 0.0969\n",
    "featureset2 RandomForestRegressor 0.0953\n",
    "featureset2 GradientBoostingRegressor 0.0759\n",
    "featureset2 SVR 0.0809\n",
    "featureset2 blending: 0.2*RandomForestRegressor + 0.8*GradientBoostingRegressor 0.0768\n",
    "featureset2 blending: 0.3*ExtraTreesRegressor + 0.7*GradientBoostingRegressor 0.0762\n",
    "featureset2 blending: 0.2*GradientBoostingRegressor + 0.8*SVR 0.0735\n",
    "featureset2 blending: 0.6*GradientBoostingRegressor + 0.4*SVR 0.0709\n",
    "blended predictions saved in ./out/prediction_blended.csv\n",
    "extended dataset saved in ./Bike-Sharing-Dataset/hour_ext.csv\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
